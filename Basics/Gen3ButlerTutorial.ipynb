{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Gen-3 Butler\n",
    "\n",
    "<br>Owners: **Alex Drlica-Wagner** ([@kadrlica](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@kadrlica)), **Douglas Tucker** ([@douglasleetucker](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@douglasleetucker))\n",
    "<br>Last Verified to Run: **2020-08-10**\n",
    "<br>Verified Stack Release: **v20.0.0**\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "This notebook provides a first look at the structure and organization of a repo created with the Gen-3 Butler. The Gen-3 Butler is still under development, so this notebook is expected to be updated after the Gen-3 release. For the time being, be sure that you are using the verified version of the stack specified above.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "This notebook lays out features of how the Gen-3 butler functions:\n",
    "\n",
    "1. Explore a Gen-3 data repo\n",
    "2. Create a Gen-3 butler\n",
    "3. Use the Gen-3 butler to explore the ci_hsc_gen3 data repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should match the verified version listed at the start of the notebook\n",
    "! eups list -s lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os,glob\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack imports\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a data repo that was run with the Gen-3 butler, I used the HSC continuous integration (CI) sample. This was generated with code like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "DATADIR=/project/shared/data\n",
    "\n",
    "#source /opt/lsst/software/stack/loadLSST.bash\n",
    "\n",
    "source /software/lsstsw/stack/loadLSST.bash\n",
    "setup lsst_distrib\n",
    "\n",
    "pushd $DATADIR/test_data/testdata_ci_hsc\n",
    "setup -j -r . \n",
    "popd\n",
    "\n",
    "rm -rf $DATADIR/ci_hsc_gen3\n",
    "git clone https://github.com/lsst/ci_hsc_gen3.git $DATADIR/ci_hsc_gen3\n",
    "\n",
    "cd $DATADIR/ci_hsc_gen3\n",
    "setup -j -r .\n",
    "\n",
    "scons --jobs 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the data repo lives\n",
    "repo='/project/shared/data/ci_hsc_gen3-w_2020_22/DATA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can poke around this directory a bit to see what outputs have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base directory for the repo\n",
    "!ls $repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outputs are stored in the `shared/ci_hsc_output`\n",
    "outdir=glob.glob(f'{repo}/shared/ci_hsc_output/*')[0]\n",
    "!ls $outdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the butler you need to pass it a configuration file and a run name. The run name tells the butler where to place output files. More on Butler configuration can be found [here](https://pipelines.lsst.io/modules/lsst.daf.butler/configuring.html). By investigating the directory structue, we find that the 'collection' is `shared/ci_hsc_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = dafButler.Butler(repo,collections=\"shared/ci_hsc_output\")\n",
    "\n",
    "# Optionally, you can specify the repo config explicitly\n",
    "#config = os.path.join(repo,'butler.yaml')\n",
    "#butler = dafButler.Butler(config=config,collections=\"shared/ci_hsc_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Gen-2 butler, there was no good way to investigate what data exist in a repo. To get around this, we all developed a habit of investigating the directory structure and file names to figure out what data existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $outdir/calexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $outdir/calexp/r/HSC-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $outdir/calexp/r/HSC-R/903338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these filenames, we have enough to specify the dataId to pass to the butler..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId = {'visit':903338,'detector':25,'instrument':'HSC'}\n",
    "calexp = butler.get('calexp', dataId=dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afwDisplay.setDefaultBackend('matplotlib') \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "afw_display = afwDisplay.Display(1)\n",
    "afw_display.scale('asinh', 'zscale')\n",
    "afw_display.mtv(calexp)\n",
    "plt.gca().axis('off')\n",
    "# And if it wasn't sacrilege I would rotate this image..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen-3 Butler\n",
    "\n",
    "Ok, so how do we do this in Gen-3 land? Starting from scratch, we want to be able to get going *with only the path to the repo*. \n",
    "\n",
    "We can now do this by creating a butler without specifying the collection (since we have no idea what collections exist at this point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = dafButler.Butler(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the butler created, we can now access the data `registry` (a database containing information about available data products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = butler.registry\n",
    "\n",
    "# We can examine the registry with\n",
    "#help(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `registry` is a good tool for investigating a repo (more on the registry schema can be found [here](https://dmtn-073.lsst.io/)). For example, we can get a list of all collections, which includes the `ci_hsc_output` collection that we were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in registry.queryCollections():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we \"know\" that `ci_hsc_output` exists, let's create our butler with this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler = dafButler.Butler(repo,collections='shared/ci_hsc_output')\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the registry to get a list of all dataset types (for example, we see that `calexp` is available, but that we could also ask directly for `calexp.image` or `calexp.mask`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in registry.queryDatasetTypes():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that this is all datasetTypes that the processing has *tried* to create during the processing. There may be intermediate products that were created during processing, but no longer exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to get all `DatasetRef` (including `dataId`) for a specific `datasetType` in a specific collection with a query like the one that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = list(registry.queryDatasets(datasetType='src',collections=['shared/ci_hsc_output']))\n",
    "for ref in datasetRefs:\n",
    "    print(ref.dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what collections exist (`shared/ci_hsc_output` in particular), the `datasetTypes` that are defined for that collection, and the `datasetRefs` (which contain `dataIds`) for data products of the requested type. This is all the information that we need to get the dataset of interest!\n",
    "\n",
    "From the list above, we find that the dataId we were investigating before has index 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataId that we found before...\n",
    "ref = datasetRefs[16]\n",
    "print(ref.dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could get the src table using the dataId as we did above for the calexp, \n",
    "# but this would require the butler to perform another query of the database. \n",
    "# Instead, we can just pass the ref itself directly to butler.get\n",
    "src = butler.get(ref)\n",
    "src = src.copy(True)\n",
    "src.asAstropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the `calexp` associated with this exposures and detector we pass the `dataId` to the butler witht the `calexp` datasetType. Note that this performs another query to the registry database to find a calexp that matches our dataId requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the calexp, we pass the dataId\n",
    "calexp = butler.get('calexp', dataId=ref.dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the calexp with the src catalog overlaid. We leave the investigation of this image as an exercise to the user :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot!\n",
    "afwDisplay.setDefaultBackend('matplotlib') \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "afw_display = afwDisplay.Display(1)\n",
    "afw_display.scale('asinh', 'zscale')\n",
    "afw_display.mtv(calexp)\n",
    "plt.gca().axis('off')\n",
    "\n",
    "with afw_display.Buffering():\n",
    "    for s in src:\n",
    "        afw_display.dot('+', s.getX(), s.getY(), ctype=afwDisplay.RED)\n",
    "        afw_display.dot('o', s.getX(), s.getY(), size=20, ctype='orange') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above, both the src and calexp can be found by the registry, but this will not necessarily be the case. The `queryDimensions` method provides a more flexible way to query for multiple datasets (requiring an instance of all datasets to be available for that dataId) or ask for different dataId keys than what is used to identify the dataset (which invokes various built-in relationships). An example of this is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use queryDimensions to provide more flexible access\n",
    "dataIds = list(registry.queryDimensions([\"exposure\", \"detector\"], datasets=[\"calexp\",\"src\"], collections=\"shared/ci_hsc_output\"))\n",
    "for dataId in dataIds:\n",
    "    print(dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we wanted to select all detectors with calexp and src datasets associated with a specific filter. We can add that constraint to our query, but first we need to figure out what the filters are called... Looking at the dataId object, we see the attributes `abstract_filter` and `physical_filter` look promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIds[0].full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"physical_filter = {dataId['physical_filter']}\")\n",
    "print(f\"abstract_filter = {dataId['abstract_filter']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `abstract_filter` is what we want, so we put it in the `where` argument of `queryDimensions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use queryDimensions to grab the dataIds for all i-band detectors\n",
    "dataIds = list(registry.queryDimensions([\"exposure\", \"detector\"], datasets=[\"calexp\",\"src\"], where=\"abstract_filter='i'\",collections=\"shared/ci_hsc_output\"))\n",
    "for dataId in dataIds:\n",
    "    print(dataId['abstract_filter'], dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Exploration\n",
    "\n",
    "Below is a scratch space for playing with things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
