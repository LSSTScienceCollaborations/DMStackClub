{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to-do\n",
    "Currently, this is a copy of Robert Lupton's RGB image demo.\n",
    "Next: figure out permissions, improve markdown comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make true-colour cutouts from HSC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.daf.persistence as dafPersist\n",
    "import lsst.afw.display.rgb as afwRgb\n",
    "import lsst.afw.geom as afwGeom\n",
    "import lsst.afw.coord as afwCoord\n",
    "import lsst.afw.image as afwImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (13, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/lsst/software/stack/stack/miniconda3-4.3.21-10a4fa6/Linux64/meas_mosaic/16.0+1/python/lsst/meas/mosaic/utils.py:30: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-27bceb58a069>\", line 1, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/opt/lsst/software/stack/python/miniconda3-4.3.21/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "dataPath = \"/datasets/hsc/repo/rerun/DM-13666/WIDE/\"\n",
    "\n",
    "butler = dafPersist.Butler(dataPath)\n",
    "skymap = butler.get(\"deepCoadd_skyMap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettypes = ['calexp', 'calexpBackground', 'icSrc',\n",
    "               'src', 'srcMatch', 'srcMatchFull', 'ossImage',\n",
    "               'flattenedImage', 'wcs', 'fcr', 'photoCalib',\n",
    "               'jointcal_wcs', 'jointcal_photoCalib', 'skyCorr',\n",
    "               'calexp_camera', 'brightObjectMask', 'deepCoadd_calexp',\n",
    "               'deepCoadd_det', 'deepCoadd_meas', 'deepCoadd_measMatch',\n",
    "               'deepCoadd_mergeDet', 'deepCoadd_ref', 'deepCoadd_forced_src',\n",
    "               'forced_src' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Butler in module lsst.daf.persistence.butler object:\n",
      "\n",
      "class Butler(builtins.object)\n",
      " |  Butler provides a generic mechanism for persisting and retrieving data using mappers.\n",
      " |  \n",
      " |  A Butler manages a collection of datasets known as a repository. Each dataset has a type representing its\n",
      " |  intended usage and a location. Note that the dataset type is not the same as the C++ or Python type of the\n",
      " |  object containing the data. For example, an ExposureF object might be used to hold the data for a raw\n",
      " |  image, a post-ISR image, a calibrated science image, or a difference image. These would all be different\n",
      " |  dataset types.\n",
      " |  \n",
      " |  A Butler can produce a collection of possible values for a key (or tuples of values for multiple keys) if\n",
      " |  given a partial data identifier. It can check for the existence of a file containing a dataset given its\n",
      " |  type and data identifier. The Butler can then retrieve the dataset. Similarly, it can persist an object to\n",
      " |  an appropriate location when given its associated data identifier.\n",
      " |  \n",
      " |  Note that the Butler has two more advanced features when retrieving a data set. First, the retrieval is\n",
      " |  lazy. Input does not occur until the data set is actually accessed. This allows datasets to be retrieved\n",
      " |  and placed on a clipboard prospectively with little cost, even if the algorithm of a stage ends up not\n",
      " |  using them. Second, the Butler will call a standardization hook upon retrieval of the dataset. This\n",
      " |  function, contained in the input mapper object, must perform any necessary manipulations to force the\n",
      " |  retrieved object to conform to standards, including translating metadata.\n",
      " |  \n",
      " |  Public methods:\n",
      " |  \n",
      " |  __init__(self, root, mapper=None, **mapperArgs)\n",
      " |  \n",
      " |  defineAlias(self, alias, datasetType)\n",
      " |  \n",
      " |  getKeys(self, datasetType=None, level=None)\n",
      " |  \n",
      " |  queryMetadata(self, datasetType, format=None, dataId={}, **rest)\n",
      " |  \n",
      " |  datasetExists(self, datasetType, dataId={}, **rest)\n",
      " |  \n",
      " |  get(self, datasetType, dataId={}, immediate=False, **rest)\n",
      " |  \n",
      " |  put(self, obj, datasetType, dataId={}, **rest)\n",
      " |  \n",
      " |  subset(self, datasetType, level=None, dataId={}, **rest)\n",
      " |  \n",
      " |  dataRef(self, datasetType, level=None, dataId={}, **rest)\n",
      " |  \n",
      " |  Initialization:\n",
      " |  \n",
      " |  The preferred method of initialization is to use the `inputs` and `outputs` __init__ parameters. These\n",
      " |  are described in the parameters section, below.\n",
      " |  \n",
      " |  For backward compatibility: this initialization method signature can take a posix root path, and\n",
      " |  optionally a mapper class instance or class type that will be instantiated using the mapperArgs input\n",
      " |  argument. However, for this to work in a backward compatible way it creates a single repository that is\n",
      " |  used as both an input and an output repository. This is NOT preferred, and will likely break any\n",
      " |  provenance system we have in place.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  root : string\n",
      " |      .. note:: Deprecated in 12_0\n",
      " |                `root` will be removed in TBD, it is replaced by `inputs` and `outputs` for\n",
      " |                multiple-repository support.\n",
      " |      A file system path. Will only work with a PosixRepository.\n",
      " |  mapper : string or instance\n",
      " |      .. note:: Deprecated in 12_0\n",
      " |                `mapper` will be removed in TBD, it is replaced by `inputs` and `outputs` for\n",
      " |                multiple-repository support.\n",
      " |      Provides a mapper to be used with Butler.\n",
      " |  mapperArgs : dict\n",
      " |      .. note:: Deprecated in 12_0\n",
      " |                `mapperArgs` will be removed in TBD, it is replaced by `inputs` and `outputs` for\n",
      " |                multiple-repository support.\n",
      " |      Provides arguments to be passed to the mapper if the mapper input argument is a class type to be\n",
      " |      instantiated by Butler.\n",
      " |  inputs : RepositoryArgs, dict, or string\n",
      " |      Can be a single item or a list. Provides arguments to load an existing repository (or repositories).\n",
      " |      String is assumed to be a URI and is used as the cfgRoot (URI to the location of the cfg file). (Local\n",
      " |      file system URI does not have to start with 'file://' and in this way can be a relative path). The\n",
      " |      `RepositoryArgs` class can be used to provide more parameters with which to initialize a repository\n",
      " |      (such as `mapper`, `mapperArgs`, `tags`, etc. See the `RepositoryArgs` documentation for more\n",
      " |      details). A dict may be used as shorthand for a `RepositoryArgs` class instance. The dict keys must\n",
      " |      match parameters to the `RepositoryArgs.__init__` function.\n",
      " |  outputs : RepositoryArgs, dict, or string\n",
      " |      Provides arguments to load one or more existing repositories or create new ones. The different types\n",
      " |      are handled the same as for `inputs`.\n",
      " |  \n",
      " |  The Butler init sequence loads all of the input and output repositories.\n",
      " |  This creates the object hierarchy to read from and write to them. Each\n",
      " |  repository can have 0 or more parents, which also get loaded as inputs.\n",
      " |  This becomes a DAG of repositories. Ultimately, Butler creates a list of\n",
      " |  these Repositories in the order that they are used.\n",
      " |  \n",
      " |  Initialization Sequence\n",
      " |  =======================\n",
      " |  \n",
      " |  During initialization Butler creates a Repository class instance & support structure for each object\n",
      " |  passed to `inputs` and `outputs` as well as the parent repositories recorded in the `RepositoryCfg` of\n",
      " |  each existing readable repository.\n",
      " |  \n",
      " |  This process is complex. It is explained below to shed some light on the intent of each step.\n",
      " |  \n",
      " |  1. Input Argument Standardization\n",
      " |  ---------------------------------\n",
      " |  \n",
      " |  In `Butler._processInputArguments` the input arguments are verified to be legal (and a RuntimeError is\n",
      " |  raised if not), and they are converted into an expected format that is used for the rest of the Butler\n",
      " |  init sequence. See the docstring for `_processInputArguments`.\n",
      " |  \n",
      " |  2. Create RepoData Objects\n",
      " |  --------------------------\n",
      " |  \n",
      " |  Butler uses an object, called `RepoData`, to keep track of information about each repository; each\n",
      " |  repository is contained in a single `RepoData`. The attributes are explained in its docstring.\n",
      " |  \n",
      " |  After `_processInputArguments`, a RepoData is instantiated and put in a list for each repository in\n",
      " |  `outputs` and `inputs`. This list of RepoData, the `repoDataList`, now represents all the output and input\n",
      " |  repositories (but not parent repositories) that this Butler instance will use.\n",
      " |  \n",
      " |  3. Get `RepositoryCfg`s\n",
      " |  -----------------------\n",
      " |  \n",
      " |  `Butler._getCfgs` gets the `RepositoryCfg` for each repository the `repoDataList`. The behavior is\n",
      " |  described in the docstring.\n",
      " |  \n",
      " |  4. Add Parents\n",
      " |  --------------\n",
      " |  \n",
      " |  `Butler._addParents` then considers the parents list in the `RepositoryCfg` of each `RepoData` in the\n",
      " |  `repoDataList` and inserts new `RepoData` objects for each parent not represented in the proper location\n",
      " |  in the `repoDataList`. Ultimately a flat list is built to represent the DAG of readable repositories\n",
      " |  represented in depth-first order.\n",
      " |  \n",
      " |  5. Set and Verify Parents of Outputs\n",
      " |  ------------------------------------\n",
      " |  \n",
      " |  To be able to load parent repositories when output repositories are used as inputs, the input repositories\n",
      " |  are recorded as parents in the `RepositoryCfg` file of new output repositories. When an output repository\n",
      " |  already exists, for consistency the Butler's inputs must match the list of parents specified the already-\n",
      " |  existing output repository's `RepositoryCfg` file.\n",
      " |  \n",
      " |  In `Butler._setAndVerifyParentsLists`, the list of parents is recorded in the `RepositoryCfg` of new\n",
      " |  repositories. For existing repositories the list of parents is compared with the `RepositoryCfg`'s parents\n",
      " |  list, and if they do not match a `RuntimeError` is raised.\n",
      " |  \n",
      " |  6. Set the Default Mapper\n",
      " |  -------------------------\n",
      " |  \n",
      " |  If all the input repositories use the same mapper then we can assume that mapper to be the\n",
      " |  \"default mapper\". If there are new output repositories whose `RepositoryArgs` do not specify a mapper and\n",
      " |  there is a default mapper then the new output repository will be set to use that default mapper.\n",
      " |  \n",
      " |  This is handled in `Butler._setDefaultMapper`.\n",
      " |  \n",
      " |  7. Cache References to Parent RepoDatas\n",
      " |  ---------------------------------------\n",
      " |  \n",
      " |  In `Butler._connectParentRepoDatas`, in each `RepoData` in `repoDataList`, a list of `RepoData` object\n",
      " |  references is  built that matches the parents specified in that `RepoData`'s `RepositoryCfg`.\n",
      " |  \n",
      " |  This list is used later to find things in that repository's parents, without considering peer repository's\n",
      " |  parents. (e.g. finding the registry of a parent)\n",
      " |  \n",
      " |  8. Set Tags\n",
      " |  -----------\n",
      " |  \n",
      " |  Tags are described at https://ldm-463.lsst.io/v/draft/#tagging\n",
      " |  \n",
      " |  In `Butler._setRepoDataTags`, for each `RepoData`, the tags specified by its `RepositoryArgs` are recorded\n",
      " |  in a set, and added to the tags set in each of its parents, for ease of lookup when mapping.\n",
      " |  \n",
      " |  9. Find Parent Registry and Instantiate RepoData\n",
      " |  ------------------------------------------------\n",
      " |  \n",
      " |  At this point there is enough information to instantiate the `Repository` instances. There is one final\n",
      " |  step before instantiating the Repository, which is to try to get a parent registry that can be used by the\n",
      " |  child repository. The criteria for \"can be used\" is spelled out in `Butler._setParentRegistry`. However,\n",
      " |  to get the registry from the parent, the parent must be instantiated. The `repoDataList`, in depth-first\n",
      " |  search order, is built so that the most-dependent repositories are first, and the least dependent\n",
      " |  repositories are last. So the `repoDataList` is reversed and the Repositories are instantiated in that\n",
      " |  order; for each RepoData a parent registry is searched for, and then the Repository is instantiated with\n",
      " |  whatever registry could be found.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root=None, mapper=None, inputs=None, outputs=None, **mapperArgs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  dataRef(self, datasetType, level=None, dataId={}, **rest)\n",
      " |      Returns a single ButlerDataRef.\n",
      " |      \n",
      " |      Given a complete dataId specified in dataId and **rest, find the unique dataset at the given level\n",
      " |      specified by a dataId key (e.g. visit or sensor or amp for a camera) and return a ButlerDataRef.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset collection to reference\n",
      " |      level - string\n",
      " |          The level of dataId at which to reference\n",
      " |      dataId - dict\n",
      " |          The data id.\n",
      " |      **rest\n",
      " |          Keyword arguments for the data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dataRef - ButlerDataRef\n",
      " |          ButlerDataRef for dataset matching the data id\n",
      " |  \n",
      " |  datasetExists(self, datasetType, dataId={}, write=False, **rest)\n",
      " |      Determines if a dataset file exists.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset to inquire about.\n",
      " |      dataId - DataId, dict\n",
      " |          The data id of the dataset.\n",
      " |      write - bool\n",
      " |          If True, look only in locations where the dataset could be written,\n",
      " |          and return True only if it is present in all of them.\n",
      " |      **rest keyword arguments for the data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      exists - bool\n",
      " |          True if the dataset exists or is non-file-based.\n",
      " |  \n",
      " |  defineAlias(self, alias, datasetType)\n",
      " |      Register an alias that will be substituted in datasetTypes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias - string\n",
      " |          The alias keyword. It may start with @ or not. It may not contain @ except as the first character.\n",
      " |      datasetType - string\n",
      " |          The string that will be substituted when @alias is passed into datasetType. It may not contain '@'\n",
      " |  \n",
      " |  get(self, datasetType, dataId=None, immediate=True, **rest)\n",
      " |      Retrieves a dataset given an input collection data id.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset to retrieve.\n",
      " |      dataId - dict\n",
      " |          The data id.\n",
      " |      immediate - bool\n",
      " |          If False use a proxy for delayed loading.\n",
      " |      **rest\n",
      " |          keyword arguments for the data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |          An object retrieved from the dataset (or a proxy for one).\n",
      " |  \n",
      " |  getKeys(self, datasetType=None, level=None, tag=None)\n",
      " |      Get the valid data id keys at or above the given level of hierarchy for the dataset type or the\n",
      " |      entire collection if None. The dict values are the basic Python types corresponding to the keys (int,\n",
      " |      float, string).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset to get keys for, entire collection if None.\n",
      " |      level - string\n",
      " |          The hierarchy level to descend to. None if it should not be restricted. Use an empty string if the\n",
      " |          mapper should lookup the default level.\n",
      " |      tags - any, or list of any\n",
      " |          Any object that can be tested to be the same as the tag in a dataId passed into butler input\n",
      " |          functions. Applies only to input repositories: If tag is specified by the dataId then the repo\n",
      " |          will only be read from used if the tag in the dataId matches a tag used for that repository.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Returns a dict. The dict keys are the valid data id keys at or above the given level of hierarchy for\n",
      " |      the dataset type or the entire collection if None. The dict values are the basic Python types\n",
      " |      corresponding to the keys (int, float, string).\n",
      " |  \n",
      " |  getUri(self, datasetType, dataId=None, write=False, **rest)\n",
      " |      Return the URI for a dataset\n",
      " |      \n",
      " |      .. warning:: This is intended only for debugging. The URI should\n",
      " |      never be used for anything other than printing.\n",
      " |      \n",
      " |      .. note:: In the event there are multiple URIs for read, we return only\n",
      " |      the first.\n",
      " |      \n",
      " |      .. note:: getUri() does not currently support composite datasets.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType : `str`\n",
      " |         The dataset type of interest.\n",
      " |      dataId : `dict`, optional\n",
      " |         The data identifier.\n",
      " |      write : `bool`, optional\n",
      " |         Return the URI for writing?\n",
      " |      rest : `dict`, optional\n",
      " |         Keyword arguments for the data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      uri : `str`\n",
      " |         URI for dataset.\n",
      " |  \n",
      " |  put(self, obj, datasetType, dataId={}, doBackup=False, **rest)\n",
      " |      Persists a dataset given an output collection data id.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      obj -\n",
      " |          The object to persist.\n",
      " |      datasetType - string\n",
      " |          The type of dataset to persist.\n",
      " |      dataId - dict\n",
      " |          The data id.\n",
      " |      doBackup - bool\n",
      " |          If True, rename existing instead of overwriting.\n",
      " |          WARNING: Setting doBackup=True is not safe for parallel processing, as it may be subject to race\n",
      " |          conditions.\n",
      " |      **rest\n",
      " |          Keyword arguments for the data id.\n",
      " |  \n",
      " |  queryMetadata(self, datasetType, format, dataId={}, **rest)\n",
      " |      Returns the valid values for one or more keys when given a partial\n",
      " |      input collection data id.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset to inquire about.\n",
      " |      format - str, tuple\n",
      " |          Key or tuple of keys to be returned.\n",
      " |      dataId - DataId, dict\n",
      " |          The partial data id.\n",
      " |      **rest -\n",
      " |          Keyword arguments for the partial data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A list of valid values or tuples of valid values as specified by the\n",
      " |      format.\n",
      " |  \n",
      " |  subset(self, datasetType, level=None, dataId={}, **rest)\n",
      " |      Return complete dataIds for a dataset type that match a partial (or empty) dataId.\n",
      " |      \n",
      " |      Given a partial (or empty) dataId specified in dataId and **rest, find all datasets that match the\n",
      " |      dataId.  Optionally restrict the results to a given level specified by a dataId key (e.g. visit or\n",
      " |      sensor or amp for a camera).  Return an iterable collection of complete dataIds as ButlerDataRefs.\n",
      " |      Datasets with the resulting dataIds may not exist; that needs to be tested with datasetExists().\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      datasetType - string\n",
      " |          The type of dataset collection to subset\n",
      " |      level - string\n",
      " |          The level of dataId at which to subset. Use an empty string if the mapper should look up the\n",
      " |          default level.\n",
      " |      dataId - dict\n",
      " |          The data id.\n",
      " |      **rest\n",
      " |          Keyword arguments for the data id.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      subset - ButlerSubset\n",
      " |          Collection of ButlerDataRefs for datasets matching the data id.\n",
      " |      \n",
      " |      Examples\n",
      " |      -----------\n",
      " |      To print the full dataIds for all r-band measurements in a source catalog\n",
      " |      (note that the subset call is equivalent to: `butler.subset('src', dataId={'filter':'r'})`):\n",
      " |      \n",
      " |      >>> subset = butler.subset('src', filter='r')\n",
      " |      >>> for data_ref in subset: print(data_ref.dataId)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  getMapperClass(root)\n",
      " |      posix-only; gets the mapper class at the path specified by root (if a file _mapper can be found at\n",
      " |      that location or in a parent location.\n",
      " |      \n",
      " |      As we abstract the storage and support different types of storage locations this method will be\n",
      " |      moved entirely into Butler Access, or made more dynamic, and the API will very likely change.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(butler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Select the object of which we want a Pretty Picture.\n",
    "\n",
    "For coadds the WCS is the same in all bands, but the code handles the general case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra, dec, name = 215.9747, -0.4344, \"Lens\"\n",
    "\n",
    "raDec = afwCoord.Coord(ra*afwGeom.degrees, dec*afwGeom.degrees)\n",
    "\n",
    "filters = \"grizy\"  # filters to process -- we choose our bands when we set B, R, G = ...\n",
    "\n",
    "cutoutSize = 500   # pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by finding the tract and patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tp in enumerate(skymap.findTractPatchList([raDec])):\n",
    "    tractInfo, patchInfo = tp\n",
    "    tract = tractInfo.getId()\n",
    "    patch = \"%d,%d\" % patchInfo[0].getIndex()\n",
    "    print i, tract, patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can read the desired pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "cutoutSize = afwGeom.ExtentI(300, 200)\n",
    "\n",
    "for f in filters:\n",
    "    filterName = \"HSC-%s\" % f.upper()\n",
    "    md = butler.get(\"deepCoadd_calexp_md\", immediate=True,\n",
    "                    tract=tract, patch=patch, filter=filterName)\n",
    "    wcs = afwImage.makeWcs(md)\n",
    "    xy = afwGeom.PointI(wcs.skyToPixel(raDec))\n",
    "\n",
    "    bbox = afwGeom.BoxI(xy - cutoutSize//2, cutoutSize)\n",
    "\n",
    "    images[f] = butler.get(\"deepCoadd_calexp_sub\", bbox=bbox, immediate=True,\n",
    "                            tract=tract, patch=patch, filter=filterName).getMaskedImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a RGB images, and optionally write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbFileFmt = \"%s-%%s.png\" % name if False else None\n",
    "if not False:\n",
    "    min = dict(gri=0.01, riz=0.01, izy=0.01)\n",
    "    max = dict(gri=0.20, riz=0.20, izy=0.25)\n",
    "else:\n",
    "    min = dict(gri=0.01, riz=0.01, izy=0.05)\n",
    "    max = dict(gri=0.20, riz=0.40, izy=0.50)\n",
    "\n",
    "Q = 10\n",
    "\n",
    "for bands in [\"gri\", \"riz\", \"izy\"]:\n",
    "    B, G, R = bands\n",
    "    rgb = afwRgb.makeRGB(images[R], images[G], images[B],\n",
    "                         min[bands], max[bands] - min[bands], Q,\n",
    "                         #saturatedBorderWidth=1, saturatedPixelValue=10\n",
    "                         )\n",
    "    \n",
    "    afwRgb.displayRGB(rgb)\n",
    "    \n",
    "    if rgbFileFmt:\n",
    "        afwRgb.writeRGB(rgbFileFmt % bands, rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
